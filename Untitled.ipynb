{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# self-class\n",
    "from model import UNet\n",
    "from dataset import *\n",
    "from util import *\n",
    "\n",
    "#Random seed\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "random_state=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## define dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, load_data, transform=None):\n",
    "        self.load_data = load_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.load_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.load_data\n",
    "\n",
    "        del data['id']\n",
    "        del data['letter']\n",
    "\n",
    "        label = data['digit']\n",
    "        input = []\n",
    "\n",
    "        for i in range(2048):\n",
    "            temp = data.iloc[i, 1:].values.reshape(28, 28)\n",
    "            input.append(temp)\n",
    "\n",
    "        # list 자료형으로는 img를 나타낼 수 없으므로 numpy 자료형으로 바꾸어 준다.\n",
    "        # 데이터 타입 에러 발생으로 np.float32를 추가해준다. (하기 정규화시 필요)\n",
    "        label = (np.array(label)).astype(np.float32)\n",
    "        input = (np.array(input)).astype(np.float32)\n",
    "\n",
    "        input = input / 255.0\n",
    "\n",
    "        # pytorch에 들어가는 dimension은 input이 3개의 축을 가져야 한다.\n",
    "        if input.ndim == 3:\n",
    "            input = input[:, :, :, np.newaxis]\n",
    "\n",
    "        # 참고> input.shape : (2048, 28, 28 ,1) / label.shape : (2048, )\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "## define transform\n",
    "\n",
    "# nnConv2D 는 nSamples x nChannels x Height x Width 의 4차원 Tensor를 입력을 기본으로 하고\n",
    "# 샘플 수에 대한 차원이 없을땐 채널, 세로, 가로만 입력해도 되는듯 하다.\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        # (2048, 28, 28, 1) → (2048, 1, 28, 28)\n",
    "        # (샘플수, 가로, 세로, 채널) → (2048, 채널, 가로, 세로)\n",
    "        # 값을 확인해봐도 자리가 바뀌는 게 아니라 순서가 바뀌어 있다.\n",
    "        input = np.moveaxis(input[:, :, :, :], -1, 1)\n",
    "\n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Normalization(object):\n",
    "    def __init__(self, mean=0.5, std=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        input = (input - self.mean) / self.std\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class RandomFlip(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.fliqud(label)\n",
    "            input = np.fliqud(input)\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## 네트워크 구축하기\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)\n",
    "\n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
    "\n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "\n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "\n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        x_regression = self.fc(dec1_1)\n",
    "\n",
    "        x = nn.Sigmoid(x_regression)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## 네트워크 저장하기\n",
    "def save(ckpt_dir, net, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
    "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
    "\n",
    "## 네트워크 불러오기\n",
    "def load(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net, optim, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0000e-03\n",
      "batch size: 8\n",
      "number of epoch: 100\n",
      "data dir: ./datasets\n",
      "ckpt dir: ./checkpoint\n",
      "log dir: ./log\n",
      "result dir: ./result\n",
      "mode: train\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kerrykim/jupyter_notebook/4. emnist/dataset.py\", line 31, in __getitem__\n    temp = data.iloc[i, 1:].values.reshape(28, 28)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1762, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2067, in _getitem_tuple\n    self._has_valid_tuple(tup)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 703, in _has_valid_tuple\n    self._validate_key(k, i)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1994, in _validate_key\n    self._validate_integer(key, axis)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2063, in _validate_integer\n    raise IndexError(\"single positional indexer is out-of-bounds\")\nIndexError: single positional indexer is out-of-bounds\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd2cea559a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# 열거하다라는 뜻, 1을 안쓰면 0부터 시작하므로 카운트가 어렵다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# 여기서 batch는 counting index이고 data는 loader_val인듯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;31m# forward pass (net에 input을 입력함으로써 forward가 시작됨)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kerrykim/jupyter_notebook/4. emnist/dataset.py\", line 31, in __getitem__\n    temp = data.iloc[i, 1:].values.reshape(28, 28)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1762, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2067, in _getitem_tuple\n    self._has_valid_tuple(tup)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 703, in _has_valid_tuple\n    self._validate_key(k, i)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1994, in _validate_key\n    self._validate_integer(key, axis)\n  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2063, in _validate_integer\n    raise IndexError(\"single positional indexer is out-of-bounds\")\nIndexError: single positional indexer is out-of-bounds\n"
     ]
    }
   ],
   "source": [
    "## 라이브러리 추가하기\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# self-class\n",
    "from model import UNet\n",
    "from dataset import *\n",
    "from util import *\n",
    "\n",
    "#Random seed\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "random_state=7\n",
    "\n",
    "## Parser 생성하기\n",
    "parser = argparse.ArgumentParser(description=\"Train the UNet\",\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--lr\", default=1e-3, type=float, dest=\"lr\")\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, dest=\"batch_size\")\n",
    "parser.add_argument(\"--num_epoch\", default=100, type=int, dest=\"num_epoch\")\n",
    "\n",
    "parser.add_argument(\"--data_dir\", default=\"./datasets\", type=str, dest=\"data_dir\")\n",
    "parser.add_argument(\"--ckpt_dir\", default=\"./checkpoint\", type=str, dest=\"ckpt_dir\")\n",
    "parser.add_argument(\"--log_dir\", default=\"./log\", type=str, dest=\"log_dir\")\n",
    "parser.add_argument(\"--result_dir\", default=\"./result\", type=str, dest=\"result_dir\")\n",
    "\n",
    "parser.add_argument(\"--t_mode\", default=\"train\", type=str, dest=\"mode\")\n",
    "parser.add_argument(\"--train_continue\", default=\"off\", type=str, dest=\"train_continue\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "## 트레이닝 파라메터 설정하기\n",
    "lr = args.lr\n",
    "batch_size = args.batch_size\n",
    "num_epoch = args.num_epoch\n",
    "\n",
    "'''\n",
    "data_dir = './datasets'\n",
    "ckpt_dir = './checkpoint'\n",
    "log_dir = './log'\n",
    "result_dir = './result'\n",
    "'''\n",
    "\n",
    "data_dir = args.data_dir\n",
    "ckpt_dir = args.ckpt_dir\n",
    "log_dir = args.log_dir\n",
    "result_dir = args.result_dir\n",
    "\n",
    "mode = args.mode\n",
    "train_continue = args.train_continue\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"learning rate: %.4e\" % lr)\n",
    "print(\"batch size: %d\" % batch_size)\n",
    "print(\"number of epoch: %d\" % num_epoch)\n",
    "print(\"data dir: %s\" % data_dir)\n",
    "print(\"ckpt dir: %s\" % ckpt_dir)\n",
    "print(\"log dir: %s\" % log_dir)\n",
    "print(\"result dir: %s\" % result_dir)\n",
    "print(\"mode: %s\" % mode)\n",
    "\n",
    "\n",
    "## 네트워크 학습하기\n",
    "if mode == 'train':\n",
    "    # 어떤 결과가 나올지 모르므로 RandomFlip()은 일단 뺀다.\n",
    "    # transform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
    "\n",
    "    transform = transforms.Compose([Normalization(mean=0.5, std=0.5), ToTensor()])\n",
    "\n",
    "    '''\n",
    "    dataset = Dataset(data_dir=os.path.join(data_dir, 'train.csv'), transform=transform)\n",
    "    label, input = dataset['label'], dataset['input'] \n",
    "    \n",
    "    이렇게 사용이 불가하다. 왜냐하면 Dataset class를 사용하면 데이터형도 dataset이 되기 때문에 dict을 사용할 수 없다.\n",
    "    '''\n",
    "\n",
    "    # train/valid dataset으로 나누어 준다.\n",
    "    # train_test_split의 output 순서\n",
    "    # 4분류 : train_input, valid_input, train_label,valid_label\n",
    "    # 2분류 : train, valid\n",
    "    load_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    train, val = train_test_split(load_data, test_size=0.125, random_state=7)\n",
    "\n",
    "    dataset_train = Dataset(train, transform=transform)\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    dataset_val = Dataset(val, transform=transform)\n",
    "    loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 그밖에 부수적인 variables 설정하기\n",
    "    num_data_train = len(train)\n",
    "    num_data_val = len(val)\n",
    "\n",
    "    num_batch_train = np.ceil(num_data_train / batch_size)  # np.ceil은 올림 함수이다. Ex> 4.2 → 5 로 변환\n",
    "    num_batch_val = np.ceil(num_data_val / batch_size)\n",
    "else:\n",
    "    transform = transforms.Compose([Normalization(mean=0.5, std=0.5), ToTensor()])\n",
    "\n",
    "    dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test.csv'), transform=transform)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    # 그밖에 부수적인 variables 설정하기\n",
    "    num_data_test = 20480\n",
    "\n",
    "    num_batch_test = np.ceil(num_data_test / batch_size)\n",
    "\n",
    "## 네트워크 생성하기\n",
    "net = UNet().to(device)\n",
    "\n",
    "## 손실함수 정의하기\n",
    "fn_loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "## Optimizer 설정하기\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "## 그밖에 부수적인 functions 설정하기\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)\n",
    "\n",
    "## 네트워크 학습시키기\n",
    "st_epoch = 0\n",
    "\n",
    "# TRAIN MODE\n",
    "if mode == 'train':\n",
    "    if train_continue == \"on\":\n",
    "        net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "    for epoch in range(st_epoch + 1, num_epoch + 1):\n",
    "        net.train()\n",
    "        loss_arr = []\n",
    "\n",
    "        # enumerate(~, 1) 에서 1은 start value를 의미한다\n",
    "        # 열거하다라는 뜻, 1을 안쓰면 0부터 시작하므로 카운트가 어렵다.\n",
    "        # 여기서 batch는 counting index이고 data는 loader_val인듯\n",
    "        for batch, data in enumerate(loader_train, 1):\n",
    "            # forward pass (net에 input을 입력함으로써 forward가 시작됨)\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "\n",
    "            output = net(input)\n",
    "\n",
    "            # backward pass\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = fn_loss(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            # 손실함수 계산\n",
    "            loss_arr += [loss.item()]\n",
    "\n",
    "            print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n",
    "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n",
    "\n",
    "        # with torch.no_grad()는 autograd를 멈추게 한다. val을 계산해야 하기 때문\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            loss_arr = []\n",
    "\n",
    "            for batch, data in enumerate(loader_val, 1):\n",
    "                # forward pass\n",
    "                label = data['label'].to(device)\n",
    "                input = data['input'].to(device)\n",
    "\n",
    "                output = net(input)\n",
    "\n",
    "                # 손실함수 계산하기\n",
    "                loss = fn_loss(output, label)\n",
    "\n",
    "                loss_arr += [loss.item()]\n",
    "\n",
    "                print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n",
    "                      (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
    "\n",
    "# TEST MODE\n",
    "else:\n",
    "    net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        loss_arr = []\n",
    "        prediction = []\n",
    "\n",
    "        for batch, data in enumerate(loader_test, 1):\n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "\n",
    "            output = net(input)\n",
    "            prediction.append(output)\n",
    "\n",
    "            # 손실함수 계산하기\n",
    "            loss = fn_loss(output, label)\n",
    "\n",
    "            loss_arr += [loss.item()]\n",
    "\n",
    "            print(\"TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
    "                  (batch, num_batch_test, np.mean(loss_arr)))\n",
    "\n",
    "        # submission\n",
    "        submission = pd.read_csv('./datasets/submission.csv')\n",
    "        submission.digit = prediction\n",
    "        submission.to_csv('submission_v1.csv', index=False)\n",
    "\n",
    "    print(\"AVERAGE TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
    "          (batch, num_batch_test, np.mean(loss_arr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda_python_3.7]",
   "language": "python",
   "name": "conda-env-cuda_python_3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
