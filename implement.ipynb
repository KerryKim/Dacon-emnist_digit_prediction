{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0000e-02\n",
      "batch size: 8\n",
      "number of epoch: 100\n",
      "data dir: ./datasets\n",
      "ckpt dir: ./checkpoint\n",
      "log dir: ./log\n",
      "result dir: ./result\n",
      "train/test_mode: train\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0224 | LOSS 2.3342\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0224 | LOSS 2.3371\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0224 | LOSS 2.3402\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0224 | LOSS 2.3335\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0224 | LOSS 2.3217\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0224 | LOSS 2.3118\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0007 / 0224 | LOSS 2.3068\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0008 / 0224 | LOSS 2.3137\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0009 / 0224 | LOSS 2.3109\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0010 / 0224 | LOSS 2.3055\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0011 / 0224 | LOSS 2.3038\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0012 / 0224 | LOSS 2.3022\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0013 / 0224 | LOSS 2.3076\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0014 / 0224 | LOSS 2.3026\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0015 / 0224 | LOSS 2.3011\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0016 / 0224 | LOSS 2.3071\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0017 / 0224 | LOSS 2.3059\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0018 / 0224 | LOSS 2.3088\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0019 / 0224 | LOSS 2.3091\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0020 / 0224 | LOSS 2.3103\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0021 / 0224 | LOSS 2.3064\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0022 / 0224 | LOSS 2.3033\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0023 / 0224 | LOSS 2.3039\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0024 / 0224 | LOSS 2.3022\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0025 / 0224 | LOSS 2.3034\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0026 / 0224 | LOSS 2.3090\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0027 / 0224 | LOSS 2.3111\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0028 / 0224 | LOSS 2.3093\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0029 / 0224 | LOSS 2.3074\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0030 / 0224 | LOSS 2.3092\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0031 / 0224 | LOSS 2.3082\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0032 / 0224 | LOSS 2.3047\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0033 / 0224 | LOSS 2.3064\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0034 / 0224 | LOSS 2.3071\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0035 / 0224 | LOSS 2.3066\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0036 / 0224 | LOSS 2.3054\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0037 / 0224 | LOSS 2.3063\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0038 / 0224 | LOSS 2.3048\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0039 / 0224 | LOSS 2.3035\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0040 / 0224 | LOSS 2.3055\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0041 / 0224 | LOSS 2.3019\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0042 / 0224 | LOSS 2.3033\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0043 / 0224 | LOSS 2.3045\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0044 / 0224 | LOSS 2.3053\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0045 / 0224 | LOSS 2.3060\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0046 / 0224 | LOSS 2.3065\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0047 / 0224 | LOSS 2.3068\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0048 / 0224 | LOSS 2.3060\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0049 / 0224 | LOSS 2.3064\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0050 / 0224 | LOSS 2.3057\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0051 / 0224 | LOSS 2.3047\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0052 / 0224 | LOSS 2.3045\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0053 / 0224 | LOSS 2.3048\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0054 / 0224 | LOSS 2.3054\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0055 / 0224 | LOSS 2.3060\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0056 / 0224 | LOSS 2.3053\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0057 / 0224 | LOSS 2.3058\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0058 / 0224 | LOSS 2.3059\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0059 / 0224 | LOSS 2.3048\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0060 / 0224 | LOSS 2.3059\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0061 / 0224 | LOSS 2.3057\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0062 / 0224 | LOSS 2.3046\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0063 / 0224 | LOSS 2.3055\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0064 / 0224 | LOSS 2.3038\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0065 / 0224 | LOSS 2.3030\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0066 / 0224 | LOSS 2.3029\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0067 / 0224 | LOSS 2.3016\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0068 / 0224 | LOSS 2.2984\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0069 / 0224 | LOSS 2.2970\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0070 / 0224 | LOSS 2.2973\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0071 / 0224 | LOSS 2.2964\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0072 / 0224 | LOSS 2.2956\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0073 / 0224 | LOSS 2.2952\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0074 / 0224 | LOSS 2.2956\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0075 / 0224 | LOSS 2.2950\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0076 / 0224 | LOSS 2.2925\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0077 / 0224 | LOSS 2.2918\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0078 / 0224 | LOSS 2.2909\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0079 / 0224 | LOSS 2.2900\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0080 / 0224 | LOSS 2.2910\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0081 / 0224 | LOSS 2.2900\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0082 / 0224 | LOSS 2.2854\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0083 / 0224 | LOSS 2.2853\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0084 / 0224 | LOSS 2.2844\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0085 / 0224 | LOSS 2.2866\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0086 / 0224 | LOSS 2.2876\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0087 / 0224 | LOSS 2.2879\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0088 / 0224 | LOSS 2.2872\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0089 / 0224 | LOSS 2.2867\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0090 / 0224 | LOSS 2.2850\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0091 / 0224 | LOSS 2.2848\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0092 / 0224 | LOSS 2.2847\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0093 / 0224 | LOSS 2.2829\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0094 / 0224 | LOSS 2.2806\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0095 / 0224 | LOSS 2.2793\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0096 / 0224 | LOSS 2.2776\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0097 / 0224 | LOSS 2.2776\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0098 / 0224 | LOSS 2.2767\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0099 / 0224 | LOSS 2.2747\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0100 / 0224 | LOSS 2.2677\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0101 / 0224 | LOSS 2.2613\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0102 / 0224 | LOSS 2.2545\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0103 / 0224 | LOSS 2.2578\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0104 / 0224 | LOSS 2.2530\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0105 / 0224 | LOSS 2.2556\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0106 / 0224 | LOSS 2.2521\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0107 / 0224 | LOSS 2.2473\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0108 / 0224 | LOSS 2.2409\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0109 / 0224 | LOSS 2.2436\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0110 / 0224 | LOSS 2.2441\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0111 / 0224 | LOSS 2.2404\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0112 / 0224 | LOSS 2.2365\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0113 / 0224 | LOSS 2.2365\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0114 / 0224 | LOSS 2.2358\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0115 / 0224 | LOSS 2.2358\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0116 / 0224 | LOSS 2.2355\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0117 / 0224 | LOSS 2.2349\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0118 / 0224 | LOSS 2.2344\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0119 / 0224 | LOSS 2.2333\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0120 / 0224 | LOSS 2.2331\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0121 / 0224 | LOSS 2.2308\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0122 / 0224 | LOSS 2.2323\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0123 / 0224 | LOSS 2.2284\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0124 / 0224 | LOSS 2.2261\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0125 / 0224 | LOSS 2.2267\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0126 / 0224 | LOSS 2.2236\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0127 / 0224 | LOSS 2.2218\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0128 / 0224 | LOSS 2.2213\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0129 / 0224 | LOSS 2.2216\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0130 / 0224 | LOSS 2.2171\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0131 / 0224 | LOSS 2.2164\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0132 / 0224 | LOSS 2.2149\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0133 / 0224 | LOSS 2.2178\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0134 / 0224 | LOSS 2.2156\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0135 / 0224 | LOSS 2.2173\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0136 / 0224 | LOSS 2.2175\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0137 / 0224 | LOSS 2.2156\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0138 / 0224 | LOSS 2.2131\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0139 / 0224 | LOSS 2.2119\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0140 / 0224 | LOSS 2.2116\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0141 / 0224 | LOSS 2.2091\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0142 / 0224 | LOSS 2.2096\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0143 / 0224 | LOSS 2.2098\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0144 / 0224 | LOSS 2.2093\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0145 / 0224 | LOSS 2.2064\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0146 / 0224 | LOSS 2.2059\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0147 / 0224 | LOSS 2.2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0148 / 0224 | LOSS 2.2045\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0149 / 0224 | LOSS 2.2020\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0150 / 0224 | LOSS 2.2016\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0151 / 0224 | LOSS 2.1985\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0152 / 0224 | LOSS 2.1958\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0153 / 0224 | LOSS 2.1898\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0154 / 0224 | LOSS 2.1925\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0155 / 0224 | LOSS 2.1931\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0156 / 0224 | LOSS 2.1885\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0157 / 0224 | LOSS 2.1852\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0158 / 0224 | LOSS 2.1841\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0159 / 0224 | LOSS 2.1821\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0160 / 0224 | LOSS 2.1816\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0161 / 0224 | LOSS 2.1798\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0162 / 0224 | LOSS 2.1764\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0163 / 0224 | LOSS 2.1758\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0164 / 0224 | LOSS 2.1740\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0165 / 0224 | LOSS 2.1734\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0166 / 0224 | LOSS 2.1741\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0167 / 0224 | LOSS 2.1697\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0168 / 0224 | LOSS 2.1694\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0169 / 0224 | LOSS 2.1696\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0170 / 0224 | LOSS 2.1685\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0171 / 0224 | LOSS 2.1657\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0172 / 0224 | LOSS 2.1635\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0173 / 0224 | LOSS 2.1650\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0174 / 0224 | LOSS 2.1670\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0175 / 0224 | LOSS 2.1645\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0176 / 0224 | LOSS 2.1645\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0177 / 0224 | LOSS 2.1619\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0178 / 0224 | LOSS 2.1616\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0179 / 0224 | LOSS 2.1589\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0180 / 0224 | LOSS 2.1582\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0181 / 0224 | LOSS 2.1567\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0182 / 0224 | LOSS 2.1556\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0183 / 0224 | LOSS 2.1548\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0184 / 0224 | LOSS 2.1519\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0185 / 0224 | LOSS 2.1526\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0186 / 0224 | LOSS 2.1504\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0187 / 0224 | LOSS 2.1523\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0188 / 0224 | LOSS 2.1512\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0189 / 0224 | LOSS 2.1517\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0190 / 0224 | LOSS 2.1507\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0191 / 0224 | LOSS 2.1497\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0192 / 0224 | LOSS 2.1513\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0193 / 0224 | LOSS 2.1474\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0194 / 0224 | LOSS 2.1466\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0195 / 0224 | LOSS 2.1459\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0196 / 0224 | LOSS 2.1449\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0197 / 0224 | LOSS 2.1448\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0198 / 0224 | LOSS 2.1447\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0199 / 0224 | LOSS 2.1420\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0200 / 0224 | LOSS 2.1396\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0201 / 0224 | LOSS 2.1376\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0202 / 0224 | LOSS 2.1343\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0203 / 0224 | LOSS 2.1317\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0204 / 0224 | LOSS 2.1288\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0205 / 0224 | LOSS 2.1305\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0206 / 0224 | LOSS 2.1289\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0207 / 0224 | LOSS 2.1287\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0208 / 0224 | LOSS 2.1277\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0209 / 0224 | LOSS 2.1240\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0210 / 0224 | LOSS 2.1237\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0211 / 0224 | LOSS 2.1224\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0212 / 0224 | LOSS 2.1210\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0213 / 0224 | LOSS 2.1199\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0214 / 0224 | LOSS 2.1191\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0215 / 0224 | LOSS 2.1184\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0216 / 0224 | LOSS 2.1165\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0217 / 0224 | LOSS 2.1154\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0218 / 0224 | LOSS 2.1142\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0219 / 0224 | LOSS 2.1131\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0220 / 0224 | LOSS 2.1099\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0221 / 0224 | LOSS 2.1092\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0222 / 0224 | LOSS 2.1111\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0223 / 0224 | LOSS 2.1079\n",
      "TRAIN: EPOCH 0001 / 0100 | BATCH 0224 / 0224 | LOSS 2.1065\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0032 | LOSS 1.5367\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0002 / 0032 | LOSS 1.6259\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0003 / 0032 | LOSS 1.5901\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0004 / 0032 | LOSS 1.6203\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0005 / 0032 | LOSS 1.7501\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0006 / 0032 | LOSS 1.7260\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0007 / 0032 | LOSS 1.7381\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0008 / 0032 | LOSS 1.7670\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0009 / 0032 | LOSS 1.7820\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0010 / 0032 | LOSS 1.7660\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0011 / 0032 | LOSS 1.7758\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0012 / 0032 | LOSS 1.8221\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0013 / 0032 | LOSS 1.8344\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0014 / 0032 | LOSS 1.8190\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0015 / 0032 | LOSS 1.8507\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0016 / 0032 | LOSS 1.8362\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0017 / 0032 | LOSS 1.7861\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0018 / 0032 | LOSS 1.8134\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0019 / 0032 | LOSS 1.8301\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0020 / 0032 | LOSS 1.8237\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0021 / 0032 | LOSS 1.8476\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0022 / 0032 | LOSS 1.8486\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0023 / 0032 | LOSS 1.8411\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0024 / 0032 | LOSS 1.8458\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0025 / 0032 | LOSS 1.8335\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0026 / 0032 | LOSS 1.8231\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0027 / 0032 | LOSS 1.8330\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0028 / 0032 | LOSS 1.8304\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0029 / 0032 | LOSS 1.8248\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0030 / 0032 | LOSS 1.8226\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0031 / 0032 | LOSS 1.8270\n",
      "VALID: EPOCH 0001 / 0100 | BATCH 0032 / 0032 | LOSS 1.8133\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0224 | LOSS 2.2499\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0224 | LOSS 1.7230\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0224 | LOSS 1.9457\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0224 | LOSS 1.8322\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0224 | LOSS 1.8548\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0224 | LOSS 1.7757\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0007 / 0224 | LOSS 1.7852\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0008 / 0224 | LOSS 1.7810\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0009 / 0224 | LOSS 1.7883\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0010 / 0224 | LOSS 1.7764\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0011 / 0224 | LOSS 1.7904\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0012 / 0224 | LOSS 1.7744\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0013 / 0224 | LOSS 1.7447\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0014 / 0224 | LOSS 1.7531\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0015 / 0224 | LOSS 1.7574\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0016 / 0224 | LOSS 1.7433\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0017 / 0224 | LOSS 1.7772\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0018 / 0224 | LOSS 1.7655\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0019 / 0224 | LOSS 1.7540\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0020 / 0224 | LOSS 1.7439\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0021 / 0224 | LOSS 1.7361\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0022 / 0224 | LOSS 1.7322\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0023 / 0224 | LOSS 1.7516\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0024 / 0224 | LOSS 1.7673\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0025 / 0224 | LOSS 1.7727\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0026 / 0224 | LOSS 1.7787\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0027 / 0224 | LOSS 1.7702\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0028 / 0224 | LOSS 1.7831\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0029 / 0224 | LOSS 1.7793\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0030 / 0224 | LOSS 1.7866\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0031 / 0224 | LOSS 1.7905\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0032 / 0224 | LOSS 1.7932\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0033 / 0224 | LOSS 1.7982\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0034 / 0224 | LOSS 1.8013\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0035 / 0224 | LOSS 1.8016\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0036 / 0224 | LOSS 1.7929\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0037 / 0224 | LOSS 1.7931\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0038 / 0224 | LOSS 1.7868\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0039 / 0224 | LOSS 1.7748\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0040 / 0224 | LOSS 1.7921\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0041 / 0224 | LOSS 1.7857\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0042 / 0224 | LOSS 1.7786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0043 / 0224 | LOSS 1.7818\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0044 / 0224 | LOSS 1.7719\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0045 / 0224 | LOSS 1.7847\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0046 / 0224 | LOSS 1.7911\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0047 / 0224 | LOSS 1.7856\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0048 / 0224 | LOSS 1.7789\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0049 / 0224 | LOSS 1.7709\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0050 / 0224 | LOSS 1.7762\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0051 / 0224 | LOSS 1.7835\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0052 / 0224 | LOSS 1.7874\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0053 / 0224 | LOSS 1.7846\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0054 / 0224 | LOSS 1.7708\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0055 / 0224 | LOSS 1.7834\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0056 / 0224 | LOSS 1.7973\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0057 / 0224 | LOSS 1.8024\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0058 / 0224 | LOSS 1.8107\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0059 / 0224 | LOSS 1.8116\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0060 / 0224 | LOSS 1.8059\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0061 / 0224 | LOSS 1.8023\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0062 / 0224 | LOSS 1.8056\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0063 / 0224 | LOSS 1.8056\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0064 / 0224 | LOSS 1.8049\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0065 / 0224 | LOSS 1.8054\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0066 / 0224 | LOSS 1.8094\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0067 / 0224 | LOSS 1.8074\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0068 / 0224 | LOSS 1.8055\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0069 / 0224 | LOSS 1.8026\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0070 / 0224 | LOSS 1.7975\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0071 / 0224 | LOSS 1.7941\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0072 / 0224 | LOSS 1.7952\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0073 / 0224 | LOSS 1.7954\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0074 / 0224 | LOSS 1.7885\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0075 / 0224 | LOSS 1.7805\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0076 / 0224 | LOSS 1.7874\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0077 / 0224 | LOSS 1.7936\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0078 / 0224 | LOSS 1.7930\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0079 / 0224 | LOSS 1.7903\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0080 / 0224 | LOSS 1.7922\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0081 / 0224 | LOSS 1.7998\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0082 / 0224 | LOSS 1.7956\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0083 / 0224 | LOSS 1.7999\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0084 / 0224 | LOSS 1.7985\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0085 / 0224 | LOSS 1.7951\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0086 / 0224 | LOSS 1.7919\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0087 / 0224 | LOSS 1.7971\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0088 / 0224 | LOSS 1.7907\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0089 / 0224 | LOSS 1.7840\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0090 / 0224 | LOSS 1.7851\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0091 / 0224 | LOSS 1.7831\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0092 / 0224 | LOSS 1.7805\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0093 / 0224 | LOSS 1.7763\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0094 / 0224 | LOSS 1.7697\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0095 / 0224 | LOSS 1.7666\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0096 / 0224 | LOSS 1.7668\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0097 / 0224 | LOSS 1.7652\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0098 / 0224 | LOSS 1.7659\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0099 / 0224 | LOSS 1.7671\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0100 / 0224 | LOSS 1.7736\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0101 / 0224 | LOSS 1.7690\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0102 / 0224 | LOSS 1.7679\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0103 / 0224 | LOSS 1.7654\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0104 / 0224 | LOSS 1.7707\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0105 / 0224 | LOSS 1.7708\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0106 / 0224 | LOSS 1.7758\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0107 / 0224 | LOSS 1.7724\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0108 / 0224 | LOSS 1.7742\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0109 / 0224 | LOSS 1.7728\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0110 / 0224 | LOSS 1.7732\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0111 / 0224 | LOSS 1.7709\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0112 / 0224 | LOSS 1.7678\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0113 / 0224 | LOSS 1.7679\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0114 / 0224 | LOSS 1.7726\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0115 / 0224 | LOSS 1.7757\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0116 / 0224 | LOSS 1.7823\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0117 / 0224 | LOSS 1.7838\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0118 / 0224 | LOSS 1.7860\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0119 / 0224 | LOSS 1.7905\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0120 / 0224 | LOSS 1.7909\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0121 / 0224 | LOSS 1.7870\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0122 / 0224 | LOSS 1.7849\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0123 / 0224 | LOSS 1.7852\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0124 / 0224 | LOSS 1.7854\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0125 / 0224 | LOSS 1.7852\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0126 / 0224 | LOSS 1.7881\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0127 / 0224 | LOSS 1.7893\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0128 / 0224 | LOSS 1.7893\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0129 / 0224 | LOSS 1.7873\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0130 / 0224 | LOSS 1.7873\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0131 / 0224 | LOSS 1.7905\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0132 / 0224 | LOSS 1.7897\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0133 / 0224 | LOSS 1.7901\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0134 / 0224 | LOSS 1.7868\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0135 / 0224 | LOSS 1.7855\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0136 / 0224 | LOSS 1.7853\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0137 / 0224 | LOSS 1.7894\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0138 / 0224 | LOSS 1.7938\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0139 / 0224 | LOSS 1.7909\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0140 / 0224 | LOSS 1.7862\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0141 / 0224 | LOSS 1.7846\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0142 / 0224 | LOSS 1.7838\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0143 / 0224 | LOSS 1.7877\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0144 / 0224 | LOSS 1.7871\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0145 / 0224 | LOSS 1.7861\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0146 / 0224 | LOSS 1.7838\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0147 / 0224 | LOSS 1.7834\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0148 / 0224 | LOSS 1.7842\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0149 / 0224 | LOSS 1.7868\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0150 / 0224 | LOSS 1.7842\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0151 / 0224 | LOSS 1.7831\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0152 / 0224 | LOSS 1.7837\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0153 / 0224 | LOSS 1.7891\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0154 / 0224 | LOSS 1.7857\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0155 / 0224 | LOSS 1.7861\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0156 / 0224 | LOSS 1.7869\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0157 / 0224 | LOSS 1.7846\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0158 / 0224 | LOSS 1.7870\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0159 / 0224 | LOSS 1.7860\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0160 / 0224 | LOSS 1.7866\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0161 / 0224 | LOSS 1.7856\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0162 / 0224 | LOSS 1.7830\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0163 / 0224 | LOSS 1.7823\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0164 / 0224 | LOSS 1.7818\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0165 / 0224 | LOSS 1.7838\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0166 / 0224 | LOSS 1.7856\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0167 / 0224 | LOSS 1.7847\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0168 / 0224 | LOSS 1.7852\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0169 / 0224 | LOSS 1.7821\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0170 / 0224 | LOSS 1.7858\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0171 / 0224 | LOSS 1.7880\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0172 / 0224 | LOSS 1.7879\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0173 / 0224 | LOSS 1.7862\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0174 / 0224 | LOSS 1.7916\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0175 / 0224 | LOSS 1.7934\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0176 / 0224 | LOSS 1.7934\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0177 / 0224 | LOSS 1.7917\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0178 / 0224 | LOSS 1.7912\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0179 / 0224 | LOSS 1.7901\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0180 / 0224 | LOSS 1.7889\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0181 / 0224 | LOSS 1.7896\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0182 / 0224 | LOSS 1.7866\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0183 / 0224 | LOSS 1.7857\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0184 / 0224 | LOSS 1.7865\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0185 / 0224 | LOSS 1.7893\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0186 / 0224 | LOSS 1.7916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0187 / 0224 | LOSS 1.7888\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0188 / 0224 | LOSS 1.7873\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0189 / 0224 | LOSS 1.7847\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0190 / 0224 | LOSS 1.7819\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0191 / 0224 | LOSS 1.7827\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0192 / 0224 | LOSS 1.7820\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0193 / 0224 | LOSS 1.7823\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0194 / 0224 | LOSS 1.7845\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0195 / 0224 | LOSS 1.7873\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0196 / 0224 | LOSS 1.7877\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0197 / 0224 | LOSS 1.7856\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0198 / 0224 | LOSS 1.7863\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0199 / 0224 | LOSS 1.7863\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0200 / 0224 | LOSS 1.7869\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0201 / 0224 | LOSS 1.7863\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0202 / 0224 | LOSS 1.7855\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0203 / 0224 | LOSS 1.7870\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0204 / 0224 | LOSS 1.7863\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0205 / 0224 | LOSS 1.7842\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0206 / 0224 | LOSS 1.7828\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0207 / 0224 | LOSS 1.7822\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0208 / 0224 | LOSS 1.7814\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0209 / 0224 | LOSS 1.7821\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0210 / 0224 | LOSS 1.7875\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0211 / 0224 | LOSS 1.7908\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0212 / 0224 | LOSS 1.7919\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0213 / 0224 | LOSS 1.7916\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0214 / 0224 | LOSS 1.7898\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0215 / 0224 | LOSS 1.7890\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0216 / 0224 | LOSS 1.7862\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0217 / 0224 | LOSS 1.7875\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0218 / 0224 | LOSS 1.7885\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0219 / 0224 | LOSS 1.7882\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0220 / 0224 | LOSS 1.7883\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0221 / 0224 | LOSS 1.7894\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0222 / 0224 | LOSS 1.7882\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0223 / 0224 | LOSS 1.7874\n",
      "TRAIN: EPOCH 0002 / 0100 | BATCH 0224 / 0224 | LOSS 1.7883\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0001 / 0032 | LOSS 1.5998\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0002 / 0032 | LOSS 1.6162\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0003 / 0032 | LOSS 1.5243\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0004 / 0032 | LOSS 1.4874\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0005 / 0032 | LOSS 1.6316\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0006 / 0032 | LOSS 1.6110\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0007 / 0032 | LOSS 1.6219\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0008 / 0032 | LOSS 1.6792\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0009 / 0032 | LOSS 1.6754\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0010 / 0032 | LOSS 1.6413\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0011 / 0032 | LOSS 1.6421\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0012 / 0032 | LOSS 1.6633\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0013 / 0032 | LOSS 1.6924\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0014 / 0032 | LOSS 1.6336\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0015 / 0032 | LOSS 1.6692\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0016 / 0032 | LOSS 1.6854\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0017 / 0032 | LOSS 1.6568\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0018 / 0032 | LOSS 1.6787\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0019 / 0032 | LOSS 1.6958\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0020 / 0032 | LOSS 1.6981\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0021 / 0032 | LOSS 1.7246\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0022 / 0032 | LOSS 1.7112\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0023 / 0032 | LOSS 1.7186\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0024 / 0032 | LOSS 1.7171\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0025 / 0032 | LOSS 1.6993\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0026 / 0032 | LOSS 1.6897\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0027 / 0032 | LOSS 1.7078\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0028 / 0032 | LOSS 1.7088\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0029 / 0032 | LOSS 1.6953\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0030 / 0032 | LOSS 1.6848\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0031 / 0032 | LOSS 1.6886\n",
      "VALID: EPOCH 0002 / 0100 | BATCH 0032 / 0032 | LOSS 1.6708\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0001 / 0224 | LOSS 2.2519\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0002 / 0224 | LOSS 1.9986\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0003 / 0224 | LOSS 2.0245\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0004 / 0224 | LOSS 1.9265\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0005 / 0224 | LOSS 1.8852\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0006 / 0224 | LOSS 1.8268\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0007 / 0224 | LOSS 1.9339\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0008 / 0224 | LOSS 1.9308\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0009 / 0224 | LOSS 1.9767\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0010 / 0224 | LOSS 1.9766\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0011 / 0224 | LOSS 1.9562\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0012 / 0224 | LOSS 1.9476\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0013 / 0224 | LOSS 1.9228\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0014 / 0224 | LOSS 1.8862\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0015 / 0224 | LOSS 1.8719\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0016 / 0224 | LOSS 1.8887\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0017 / 0224 | LOSS 1.8706\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0018 / 0224 | LOSS 1.8577\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0019 / 0224 | LOSS 1.8676\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0020 / 0224 | LOSS 1.8922\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0021 / 0224 | LOSS 1.8862\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0022 / 0224 | LOSS 1.8566\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0023 / 0224 | LOSS 1.8464\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0024 / 0224 | LOSS 1.8425\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0025 / 0224 | LOSS 1.8468\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0026 / 0224 | LOSS 1.8458\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0027 / 0224 | LOSS 1.8424\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0028 / 0224 | LOSS 1.8240\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0029 / 0224 | LOSS 1.8360\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0030 / 0224 | LOSS 1.8230\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0031 / 0224 | LOSS 1.8059\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0032 / 0224 | LOSS 1.8038\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0033 / 0224 | LOSS 1.8062\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0034 / 0224 | LOSS 1.8009\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0035 / 0224 | LOSS 1.8025\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0036 / 0224 | LOSS 1.8024\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0037 / 0224 | LOSS 1.7856\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0038 / 0224 | LOSS 1.7828\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0039 / 0224 | LOSS 1.7890\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0040 / 0224 | LOSS 1.7929\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0041 / 0224 | LOSS 1.8014\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0042 / 0224 | LOSS 1.7868\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0043 / 0224 | LOSS 1.7728\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0044 / 0224 | LOSS 1.7595\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0045 / 0224 | LOSS 1.7432\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0046 / 0224 | LOSS 1.7416\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0047 / 0224 | LOSS 1.7309\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0048 / 0224 | LOSS 1.7296\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0049 / 0224 | LOSS 1.7237\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0050 / 0224 | LOSS 1.7170\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0051 / 0224 | LOSS 1.7377\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0052 / 0224 | LOSS 1.7361\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0053 / 0224 | LOSS 1.7410\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0054 / 0224 | LOSS 1.7444\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0055 / 0224 | LOSS 1.7413\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0056 / 0224 | LOSS 1.7413\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0057 / 0224 | LOSS 1.7449\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0058 / 0224 | LOSS 1.7442\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0059 / 0224 | LOSS 1.7425\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0060 / 0224 | LOSS 1.7420\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0061 / 0224 | LOSS 1.7363\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0062 / 0224 | LOSS 1.7268\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0063 / 0224 | LOSS 1.7258\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0064 / 0224 | LOSS 1.7202\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0065 / 0224 | LOSS 1.7317\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0066 / 0224 | LOSS 1.7444\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0067 / 0224 | LOSS 1.7388\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0068 / 0224 | LOSS 1.7337\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0069 / 0224 | LOSS 1.7319\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0070 / 0224 | LOSS 1.7286\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0071 / 0224 | LOSS 1.7236\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0072 / 0224 | LOSS 1.7180\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0073 / 0224 | LOSS 1.7153\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0074 / 0224 | LOSS 1.7106\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0075 / 0224 | LOSS 1.7115\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0076 / 0224 | LOSS 1.7049\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0077 / 0224 | LOSS 1.7049\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0078 / 0224 | LOSS 1.7123\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0079 / 0224 | LOSS 1.7081\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0080 / 0224 | LOSS 1.7086\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0081 / 0224 | LOSS 1.7124\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0082 / 0224 | LOSS 1.7114\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0083 / 0224 | LOSS 1.7163\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0084 / 0224 | LOSS 1.7106\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0085 / 0224 | LOSS 1.7056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0086 / 0224 | LOSS 1.7001\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0087 / 0224 | LOSS 1.6952\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0088 / 0224 | LOSS 1.7052\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0089 / 0224 | LOSS 1.7034\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0090 / 0224 | LOSS 1.6946\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0091 / 0224 | LOSS 1.6938\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0092 / 0224 | LOSS 1.6865\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0093 / 0224 | LOSS 1.6888\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0094 / 0224 | LOSS 1.6978\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0095 / 0224 | LOSS 1.6958\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0096 / 0224 | LOSS 1.6962\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0097 / 0224 | LOSS 1.6957\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0098 / 0224 | LOSS 1.6976\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0099 / 0224 | LOSS 1.6901\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0100 / 0224 | LOSS 1.6971\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0101 / 0224 | LOSS 1.7018\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0102 / 0224 | LOSS 1.6961\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0103 / 0224 | LOSS 1.7023\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0104 / 0224 | LOSS 1.6997\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0105 / 0224 | LOSS 1.6992\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0106 / 0224 | LOSS 1.6965\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0107 / 0224 | LOSS 1.6973\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0108 / 0224 | LOSS 1.6934\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0109 / 0224 | LOSS 1.6947\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0110 / 0224 | LOSS 1.6936\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0111 / 0224 | LOSS 1.6899\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0112 / 0224 | LOSS 1.6830\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0113 / 0224 | LOSS 1.6842\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0114 / 0224 | LOSS 1.6864\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0115 / 0224 | LOSS 1.6855\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0116 / 0224 | LOSS 1.6877\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0117 / 0224 | LOSS 1.6893\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0118 / 0224 | LOSS 1.6927\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0119 / 0224 | LOSS 1.6967\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0120 / 0224 | LOSS 1.6983\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0121 / 0224 | LOSS 1.6962\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0122 / 0224 | LOSS 1.6924\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0123 / 0224 | LOSS 1.6898\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0124 / 0224 | LOSS 1.6881\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0125 / 0224 | LOSS 1.6906\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0126 / 0224 | LOSS 1.6897\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0127 / 0224 | LOSS 1.6900\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0128 / 0224 | LOSS 1.6914\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0129 / 0224 | LOSS 1.6913\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0130 / 0224 | LOSS 1.6896\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0131 / 0224 | LOSS 1.6883\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0132 / 0224 | LOSS 1.6856\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0133 / 0224 | LOSS 1.6895\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0134 / 0224 | LOSS 1.6884\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0135 / 0224 | LOSS 1.6855\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0136 / 0224 | LOSS 1.6870\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0137 / 0224 | LOSS 1.6851\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0138 / 0224 | LOSS 1.6814\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0139 / 0224 | LOSS 1.6804\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0140 / 0224 | LOSS 1.6804\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0141 / 0224 | LOSS 1.6833\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0142 / 0224 | LOSS 1.6802\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0143 / 0224 | LOSS 1.6739\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0144 / 0224 | LOSS 1.6736\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0145 / 0224 | LOSS 1.6695\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0146 / 0224 | LOSS 1.6670\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0147 / 0224 | LOSS 1.6641\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0148 / 0224 | LOSS 1.6642\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0149 / 0224 | LOSS 1.6624\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0150 / 0224 | LOSS 1.6621\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0151 / 0224 | LOSS 1.6568\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0152 / 0224 | LOSS 1.6570\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0153 / 0224 | LOSS 1.6561\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0154 / 0224 | LOSS 1.6562\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0155 / 0224 | LOSS 1.6563\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0156 / 0224 | LOSS 1.6545\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0157 / 0224 | LOSS 1.6519\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0158 / 0224 | LOSS 1.6544\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0159 / 0224 | LOSS 1.6526\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0160 / 0224 | LOSS 1.6512\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0161 / 0224 | LOSS 1.6512\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0162 / 0224 | LOSS 1.6514\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0163 / 0224 | LOSS 1.6472\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0164 / 0224 | LOSS 1.6524\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0165 / 0224 | LOSS 1.6540\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0166 / 0224 | LOSS 1.6560\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0167 / 0224 | LOSS 1.6569\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0168 / 0224 | LOSS 1.6594\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0169 / 0224 | LOSS 1.6605\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0170 / 0224 | LOSS 1.6588\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0171 / 0224 | LOSS 1.6589\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0172 / 0224 | LOSS 1.6574\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0173 / 0224 | LOSS 1.6575\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0174 / 0224 | LOSS 1.6583\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0175 / 0224 | LOSS 1.6575\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0176 / 0224 | LOSS 1.6586\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0177 / 0224 | LOSS 1.6601\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0178 / 0224 | LOSS 1.6584\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0179 / 0224 | LOSS 1.6575\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0180 / 0224 | LOSS 1.6540\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0181 / 0224 | LOSS 1.6514\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0182 / 0224 | LOSS 1.6533\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0183 / 0224 | LOSS 1.6556\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0184 / 0224 | LOSS 1.6575\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0185 / 0224 | LOSS 1.6559\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0186 / 0224 | LOSS 1.6530\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0187 / 0224 | LOSS 1.6532\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0188 / 0224 | LOSS 1.6535\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0189 / 0224 | LOSS 1.6516\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0190 / 0224 | LOSS 1.6504\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0191 / 0224 | LOSS 1.6499\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0192 / 0224 | LOSS 1.6489\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0193 / 0224 | LOSS 1.6500\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0194 / 0224 | LOSS 1.6522\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0195 / 0224 | LOSS 1.6508\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0196 / 0224 | LOSS 1.6518\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0197 / 0224 | LOSS 1.6503\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0198 / 0224 | LOSS 1.6511\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0199 / 0224 | LOSS 1.6487\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0200 / 0224 | LOSS 1.6474\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0201 / 0224 | LOSS 1.6465\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0202 / 0224 | LOSS 1.6442\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0203 / 0224 | LOSS 1.6435\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0204 / 0224 | LOSS 1.6422\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0205 / 0224 | LOSS 1.6415\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0206 / 0224 | LOSS 1.6382\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0207 / 0224 | LOSS 1.6397\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0208 / 0224 | LOSS 1.6388\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0209 / 0224 | LOSS 1.6387\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0210 / 0224 | LOSS 1.6358\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0211 / 0224 | LOSS 1.6331\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0212 / 0224 | LOSS 1.6308\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0213 / 0224 | LOSS 1.6296\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0214 / 0224 | LOSS 1.6311\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0215 / 0224 | LOSS 1.6315\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0216 / 0224 | LOSS 1.6370\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0217 / 0224 | LOSS 1.6380\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0218 / 0224 | LOSS 1.6398\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0219 / 0224 | LOSS 1.6407\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0220 / 0224 | LOSS 1.6410\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0221 / 0224 | LOSS 1.6410\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0222 / 0224 | LOSS 1.6398\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0223 / 0224 | LOSS 1.6395\n",
      "TRAIN: EPOCH 0003 / 0100 | BATCH 0224 / 0224 | LOSS 1.6396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID: EPOCH 0003 / 0100 | BATCH 0001 / 0032 | LOSS 1.4223\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0002 / 0032 | LOSS 1.6105\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0003 / 0032 | LOSS 1.6324\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0004 / 0032 | LOSS 1.5552\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0005 / 0032 | LOSS 1.7126\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0006 / 0032 | LOSS 1.7367\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0007 / 0032 | LOSS 1.7170\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0008 / 0032 | LOSS 1.7152\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0009 / 0032 | LOSS 1.7018\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0010 / 0032 | LOSS 1.7019\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0011 / 0032 | LOSS 1.6616\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0012 / 0032 | LOSS 1.6755\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0013 / 0032 | LOSS 1.6785\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0014 / 0032 | LOSS 1.6562\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0015 / 0032 | LOSS 1.6664\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0016 / 0032 | LOSS 1.6893\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0017 / 0032 | LOSS 1.6445\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0018 / 0032 | LOSS 1.6646\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0019 / 0032 | LOSS 1.6882\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0020 / 0032 | LOSS 1.6741\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0021 / 0032 | LOSS 1.6987\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0022 / 0032 | LOSS 1.6899\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0023 / 0032 | LOSS 1.6696\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0024 / 0032 | LOSS 1.6716\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0025 / 0032 | LOSS 1.6672\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0026 / 0032 | LOSS 1.6501\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0027 / 0032 | LOSS 1.6676\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0028 / 0032 | LOSS 1.6626\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0029 / 0032 | LOSS 1.6470\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0030 / 0032 | LOSS 1.6413\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0031 / 0032 | LOSS 1.6487\n",
      "VALID: EPOCH 0003 / 0100 | BATCH 0032 / 0032 | LOSS 1.6322\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0001 / 0224 | LOSS 1.7516\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0002 / 0224 | LOSS 1.6167\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0003 / 0224 | LOSS 1.6089\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0004 / 0224 | LOSS 1.5768\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0005 / 0224 | LOSS 1.5855\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0006 / 0224 | LOSS 1.5582\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0007 / 0224 | LOSS 1.5521\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0008 / 0224 | LOSS 1.6300\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0009 / 0224 | LOSS 1.7242\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0010 / 0224 | LOSS 1.6767\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0011 / 0224 | LOSS 1.7028\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0012 / 0224 | LOSS 1.7475\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0013 / 0224 | LOSS 1.6940\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0014 / 0224 | LOSS 1.6923\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0015 / 0224 | LOSS 1.6743\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0016 / 0224 | LOSS 1.6628\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0017 / 0224 | LOSS 1.6555\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0018 / 0224 | LOSS 1.6414\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0019 / 0224 | LOSS 1.6321\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0020 / 0224 | LOSS 1.6344\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0021 / 0224 | LOSS 1.6444\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0022 / 0224 | LOSS 1.6567\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0023 / 0224 | LOSS 1.6502\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0024 / 0224 | LOSS 1.6608\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0025 / 0224 | LOSS 1.6429\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0026 / 0224 | LOSS 1.6054\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0027 / 0224 | LOSS 1.6076\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0028 / 0224 | LOSS 1.6188\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0029 / 0224 | LOSS 1.6120\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0030 / 0224 | LOSS 1.6006\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0031 / 0224 | LOSS 1.5981\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0032 / 0224 | LOSS 1.5977\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0033 / 0224 | LOSS 1.6115\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0034 / 0224 | LOSS 1.6088\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0035 / 0224 | LOSS 1.5994\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0036 / 0224 | LOSS 1.5950\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0037 / 0224 | LOSS 1.5891\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0038 / 0224 | LOSS 1.5797\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0039 / 0224 | LOSS 1.5967\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0040 / 0224 | LOSS 1.5857\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0041 / 0224 | LOSS 1.5872\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0042 / 0224 | LOSS 1.5942\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0043 / 0224 | LOSS 1.5824\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0044 / 0224 | LOSS 1.5726\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0045 / 0224 | LOSS 1.5538\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0046 / 0224 | LOSS 1.5494\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0047 / 0224 | LOSS 1.5588\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0048 / 0224 | LOSS 1.5488\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0049 / 0224 | LOSS 1.5481\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0050 / 0224 | LOSS 1.5465\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0051 / 0224 | LOSS 1.5454\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0052 / 0224 | LOSS 1.5456\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0053 / 0224 | LOSS 1.5497\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0054 / 0224 | LOSS 1.5379\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0055 / 0224 | LOSS 1.5532\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0056 / 0224 | LOSS 1.5466\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0057 / 0224 | LOSS 1.5461\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0058 / 0224 | LOSS 1.5519\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0059 / 0224 | LOSS 1.5557\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0060 / 0224 | LOSS 1.5548\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0061 / 0224 | LOSS 1.5467\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0062 / 0224 | LOSS 1.5467\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0063 / 0224 | LOSS 1.5531\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0064 / 0224 | LOSS 1.5438\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0065 / 0224 | LOSS 1.5442\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0066 / 0224 | LOSS 1.5387\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0067 / 0224 | LOSS 1.5354\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0068 / 0224 | LOSS 1.5360\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0069 / 0224 | LOSS 1.5396\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0070 / 0224 | LOSS 1.5361\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0071 / 0224 | LOSS 1.5419\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0072 / 0224 | LOSS 1.5471\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0073 / 0224 | LOSS 1.5438\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0074 / 0224 | LOSS 1.5351\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0075 / 0224 | LOSS 1.5388\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0076 / 0224 | LOSS 1.5450\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0077 / 0224 | LOSS 1.5444\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0078 / 0224 | LOSS 1.5445\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0079 / 0224 | LOSS 1.5450\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0080 / 0224 | LOSS 1.5502\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0081 / 0224 | LOSS 1.5443\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0082 / 0224 | LOSS 1.5347\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0083 / 0224 | LOSS 1.5352\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0084 / 0224 | LOSS 1.5279\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0085 / 0224 | LOSS 1.5277\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0086 / 0224 | LOSS 1.5266\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0087 / 0224 | LOSS 1.5210\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0088 / 0224 | LOSS 1.5306\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0089 / 0224 | LOSS 1.5281\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0090 / 0224 | LOSS 1.5261\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0091 / 0224 | LOSS 1.5225\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0092 / 0224 | LOSS 1.5256\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0093 / 0224 | LOSS 1.5203\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0094 / 0224 | LOSS 1.5170\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0095 / 0224 | LOSS 1.5176\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0096 / 0224 | LOSS 1.5218\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0097 / 0224 | LOSS 1.5176\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0098 / 0224 | LOSS 1.5138\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0099 / 0224 | LOSS 1.5163\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0100 / 0224 | LOSS 1.5144\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0101 / 0224 | LOSS 1.5083\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0102 / 0224 | LOSS 1.5088\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0103 / 0224 | LOSS 1.5096\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0104 / 0224 | LOSS 1.5075\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0105 / 0224 | LOSS 1.5055\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0106 / 0224 | LOSS 1.5036\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0107 / 0224 | LOSS 1.5025\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0108 / 0224 | LOSS 1.5007\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0109 / 0224 | LOSS 1.4968\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0110 / 0224 | LOSS 1.5026\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0111 / 0224 | LOSS 1.4999\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0112 / 0224 | LOSS 1.5039\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0113 / 0224 | LOSS 1.5091\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0114 / 0224 | LOSS 1.5096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0115 / 0224 | LOSS 1.5047\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0116 / 0224 | LOSS 1.5028\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0117 / 0224 | LOSS 1.5027\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0118 / 0224 | LOSS 1.5024\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0119 / 0224 | LOSS 1.5062\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0120 / 0224 | LOSS 1.5036\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0121 / 0224 | LOSS 1.5054\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0122 / 0224 | LOSS 1.5120\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0123 / 0224 | LOSS 1.5109\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0124 / 0224 | LOSS 1.5077\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0125 / 0224 | LOSS 1.5050\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0126 / 0224 | LOSS 1.5056\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0127 / 0224 | LOSS 1.5064\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0128 / 0224 | LOSS 1.5077\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0129 / 0224 | LOSS 1.5023\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0130 / 0224 | LOSS 1.5050\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0131 / 0224 | LOSS 1.5054\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0132 / 0224 | LOSS 1.5091\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0133 / 0224 | LOSS 1.5104\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0134 / 0224 | LOSS 1.5103\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0135 / 0224 | LOSS 1.5092\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0136 / 0224 | LOSS 1.5088\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0137 / 0224 | LOSS 1.5094\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0138 / 0224 | LOSS 1.5074\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0139 / 0224 | LOSS 1.5091\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0140 / 0224 | LOSS 1.5106\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0141 / 0224 | LOSS 1.5107\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0142 / 0224 | LOSS 1.5094\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0143 / 0224 | LOSS 1.5092\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0144 / 0224 | LOSS 1.5087\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0145 / 0224 | LOSS 1.5113\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0146 / 0224 | LOSS 1.5157\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0147 / 0224 | LOSS 1.5152\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0148 / 0224 | LOSS 1.5181\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0149 / 0224 | LOSS 1.5164\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0150 / 0224 | LOSS 1.5165\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0151 / 0224 | LOSS 1.5143\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0152 / 0224 | LOSS 1.5160\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0160 / 0224 | LOSS 1.5269\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0161 / 0224 | LOSS 1.5244\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0162 / 0224 | LOSS 1.5260\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0163 / 0224 | LOSS 1.5212\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0164 / 0224 | LOSS 1.5165\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0165 / 0224 | LOSS 1.5145\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0166 / 0224 | LOSS 1.5159\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0167 / 0224 | LOSS 1.5159\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0168 / 0224 | LOSS 1.5146\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0169 / 0224 | LOSS 1.5125\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0170 / 0224 | LOSS 1.5125\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0171 / 0224 | LOSS 1.5136\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0172 / 0224 | LOSS 1.5208\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0173 / 0224 | LOSS 1.5197\n",
      "TRAIN: EPOCH 0004 / 0100 | BATCH 0174 / 0224 | LOSS 1.5213\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./train.py\", line 168, in <module>\n",
      "    optim.step()\n",
      "  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kerrykim/anaconda3/envs/cuda_python_3.7/lib/python3.7/site-packages/torch/optim/adam.py\", line 99, in step\n",
      "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 './train.py' \\\n",
    "--lr 1e-2 --batch_size 8 --num_epoch 100 \\\n",
    "--data_dir \"./datasets\" \\\n",
    "--ckpt_dir \"./checkpoint\" \\\n",
    "--log_dir \"./log\" \\\n",
    "--result_dir \"./result\" \\\n",
    "--train/test_mode \"train\" \\\n",
    "--train_continue \"off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda_python_3.7]",
   "language": "python",
   "name": "conda-env-cuda_python_3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
